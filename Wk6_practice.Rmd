---
title: "Wk6_practice"
output: html_document
date: "2024-11-12"
---


```{r}
#first library a few packages that we will use during the practical
#note you may need to install them first...

library(spatstat)
library(here)
library(sp)
library(tmap)
library(sf)
library(tmaptools)

```


```{r}


##First, get the London Borough Boundaries
LondonBoroughs <- st_read(here::here("/Users/shinliu/Desktop/CASA0005/week 5/Wk5-prac", "statistical-gis-boundaries-london", "ESRI", "London_Borough_Excluding_MHW.shp"))


# Or use this to read in directly.
#LondonBoroughs <- st_read("https://opendata.arcgis.com/datasets/8edafbe3276d4b56aec60991cbddda50_4.geojson")

```

```{r}

library(stringr)
BoroughMap <- LondonBoroughs %>%
  dplyr::filter(str_detect(GSS_CODE, "^E09"))%>%
  st_transform(., 27700)

qtm(BoroughMap)


```

```{r}

summary(BoroughMap)
```
```{r}
## Now get the location of all Blue Plaques in the City
BluePlaques <- st_read("https://s3.eu-west-2.amazonaws.com/openplaques/open-plaques-london-2018-04-08.geojson")%>%
  st_transform(.,27700)

# *** BluePlaques --- Geodetic CRS:  WGS 84 need to transform to 27700
  
```

```{r}
summary(BluePlaques)

```


```{r}

#plot the blue plaques in the city
tmap_mode("plot")
tm_shape(BoroughMap) +
  tm_polygons(col = NA, alpha = 0.5) +
tm_shape(BluePlaques) +
  tm_dots(col = "blue")

```

## 6.5.1 Data cleaning

you might have noticed that there is at least one Blue Plaque that falls outside of the Borough boundaries. Errant plaques will cause problems with our analysis, so we need to clip the plaques to the boundaries…First we’ll remove any Plaques with the same grid reference as this will cause problems later on in the analysis.

```{r}
#remove duplicates

library(tidyverse)
library(sf)
BluePlaques <- distinct(BluePlaques)

```

## 6.5.2 Spatial subsetting
Now just select the points inside London. 


```{r}

BluePlaquesSub <- BluePlaques[BoroughMap,]
## Here, the second operator is blank , , - this controls which attributes are kept, although I’d rather keep all of them and manipulate with the tidyverse.

##OR 
## When we spatial subset data like this there are different topological relations we can specify. The default is intersects, but we could also use BluePlaquesSub <- BluePlaques[BoroughMap, , op = st_within], with the operator or op set to st_within, to identify points completely within the borough outline
#BluePlaquesSub <- BluePlaques[BoroughMap, , op = st_within]

## OR
## We can also just use the function which will have the indices of where they intersect.
#intersect_indices <-st_intersects(BoroughMap, BluePlaques)



#check to see that they've been removed
tmap_mode("plot")
tm_shape(BoroughMap) +
  tm_polygons(col = NA, alpha = 0.5) +
tm_shape(BluePlaquesSub) +
  tm_dots(col = "blue")



```




## 6.5.3 Spatial clipping
Recall from last week we also touched on spatial joining….well using the same example what if i changed the left dataset (in the left join) to the OSM data. We see that there are 715 rows compared to 712 from last week. This is because the hotels outside London are being included as all data on the left is retained….

```{r}

Londonborough <- st_read(here::here("/Users/shinliu/Desktop/CASA0005/week 5/Wk5-prac", "statistical-gis-boundaries-london", "ESRI", "London_Borough_Excluding_MHW.shp"))%>%
  st_transform(., 27700)

```

```{r}
```


```{r}
OSM <- st_read(here::here("/Users/shinliu/Desktop/CASA0005/week 5/Wk5-prac",
                          "greater-london-latest-free.shp", 
                          "gis_osm_pois_a_free_1.shp")) %>%
  st_transform(., 27700) %>%
  #select hotels only
  filter(fclass == 'hotel')
```
```{r}

join_example <-  st_join(OSM, Londonborough)

nrow(join_example)

```



In the example from last week I:

    Loaded in OSM and airbnb data
    Counted it within each London Borough
    Joined the two sf datasets to make one sf object that had the number of hotels and airbnb per borough…specifically this was for the advanced interactive map i made.

Let’s get to the point where i joined the two datasets…I’ve removed some of the other mapping code…
```{r}

OSM <- st_read(here::here("/Users/shinliu/Desktop/CASA0005/week 5/Wk5-prac",
                          "greater-london-latest-free.shp", 
                          "gis_osm_pois_a_free_1.shp")) %>%
  st_transform(., 27700) %>%
  #select hotels only
  filter(fclass == 'hotel')

```
```{r}
#London Borough data is already in 277000

Londonborough <- st_read(here::here(
  "/Users/shinliu/Desktop/CASA0005/week 5/Wk5-prac", 
  "statistical-gis-boundaries-london", 
  "ESRI", 
  "London_Borough_Excluding_MHW.shp")) %>% 
  st_transform(., 27700)


```
```{r}

# read in the .csv
# and make it into spatial data

Airbnb <- read_csv(here::here("/Users/shinliu/Desktop/CASA0005/week 5/Wk5-prac","listings.csv"))%>%
  st_as_sf(., coords = c("longitude", "latitude"), 
                   crs = 4326) %>%
    st_transform(., 27700)%>%
    #select entire places that are available all year
    filter(room_type == 'Entire home/apt' & availability_365 =='365')


# make a function for the join
# functions are covered in practical 7
# but see if you can work out what is going on
# hint all you have to do is replace data1 and data2
# with the data you want to use

Joinfun <- function(data1, data2){

output<- data1%>%
  st_join(Londonborough,.) %>%
  add_count(GSS_CODE, name="hotels_in_borough") 

  return(output)
}

# use the function for hotels
Hotels <- Joinfun(OSM, Londonborough)

# then for airbnb
Airbnb <- Joinfun(Airbnb, Londonborough)
 
Hotels <- Hotels %>%
  #at the moment each hotel is a row for the borough
  #we just one one row that has number of airbnbs
  group_by(., GSS_CODE, NAME)%>%
  summarise(`Accomodation count` = unique(hotels_in_borough))

Airbnb <- Airbnb %>%
  group_by(., GSS_CODE, NAME)%>%
  summarise(`Accomodation count` = unique(hotels_in_borough))



```

# At this point I have

    Airbnb sf object = airbnbs per borough
    Hotels sf object = hotels per borough



## Join Data 
Now how do i join them? Well i could try and use a left_join() but it won’t work with spatial data…the error message says to use st_join()
```{r}
 left_join<- left_join(Hotels, Airbnb,
            by = c("GSS_CODE" = "GSS_CODE"))
```

Ok, so st_join()
```{r}

all_accomodation <- st_join(Hotels, Airbnb)

head(all_accomodation)

```
Now this produces something that looks wrong, as the first 6 rows are all for the City of London, but recall:

    st_join is a left join, here the left table is hotels
    st_join uses st_intersects() by default. This means for the hotel data, where the airbnb intersects each hotel borough a new row is added. Conceptually, this is the same as a left join if there are multiple matches (but the matches here are defined by st_intersects not a column ID)…
    To further explain this open the all_accomodation and look at the .y columns, any borough that intersects another is added as a row …

## To rectify this we can change the argument to st_equals() that returns the data based on the same geometries (or polygons)….
```{r}

all_accomodation <- st_join(Hotels, Airbnb, join = st_equals)

head(all_accomodation)

```


##### 6.5.6 Study area
From this point, we could try and carry out our analysis on the whole of London, but you might be waiting until next week for Ripley’s K to be calculated for this many points. Therefore to speed things up and to enable us to compare areas within London, we will select some individual boroughs. 

First we need to subset our sf object to pull out a borough we are interested in. I’m going to choose Harrow as I know there are few enough points for the analysis to definitely work. 

If you wish, feel free to choose another borough in London and run the same analysis, but beware that if it happens that there are a lot of blue plaques in your borough, the analysis could fall over!!

```{r}

#extract the borough

# select by attribute
Harrow <- BoroughMap %>%
  filter(., NAME=="Harrow")

#Check to see that the correct borough has been pulled out
tm_shape(Harrow) +
  tm_polygons(col = NA, alpha = 0.5)
  
```

Next we need to clip our Blue Plaques so that we have a subset of just those that fall within the borough or interest

```{r}

#clip the data to our single borough
BluePlaquesSub <- BluePlaques[Harrow,]

#check that it's worked
tmap_mode("plot")


tm_shape(Harrow) +
  tm_polygons(col = NA, alpha = 0.5) +
tm_shape(BluePlaquesSub) +
  tm_dots(col = "blue")

```

##  start the analysis using spatstat. 
We now have all of our data set up so that we can start the analysis using spatstat. 

The first thing we need to do is create an observation window for spatstat to carry out its analysis within — we’ll set this to the extent of the Harrow boundary

```{r}

#now set a window as the borough boundary
window <- as.owin(Harrow)
plot(window)

```
spatstat has its own set of spatial objects that it works with (one of the delights of R is that different packages are written by different people and many have developed their own data types) — it does not work directly with the SpatialPolygonsDataFrames, SpatialPointsDataFrames or sf objects that we are used to. 

## For point pattern analysis, we need to create a point pattern (ppp) object.
```{r}

#create a sp object
BluePlaquesSub<- BluePlaquesSub %>%
  as(., 'Spatial')

#create a ppp object
BluePlaquesSub.ppp <- ppp(x=BluePlaquesSub@coords[,1],
                          y=BluePlaquesSub@coords[,2],
                          window=window)




## Try to understand what the different elements in command above is doing. If you are unsure, you can run elements of the code, for example:
BluePlaquesSub@coords[,1]




# Have a look at the new ppp object
BluePlaquesSub.ppp %>%
  plot(.,pch=16,cex=0.5, 
       main="Blue Plaques Harrow")


```



# 6.6 Point pattern analysis



## 6.6.1 Kernel Density Estimation
One way to summarise your point data is to plot the density of your points under a window called a ‘Kernel’. The size and shape of the Kernel affects the density pattern produced, but it is very easy to produce a Kernel Density Estimation (KDE) map from a ppp object using the density() function.

```{r}

# produce a Kernel Density Estimation (KDE) map from a ppp object using the density() function.
BluePlaquesSub.ppp %>%
  density(., sigma=500) %>%
  plot()

```

```{r}

## The sigma value sets the diameter of the Kernel (in the units your map is in — in this case, as we are in British National Grid the units are in metres). 

## Try experimenting with different values of sigma to see how that affects the density estimate.
BluePlaquesSub.ppp %>%
  density(., sigma=1000) %>%
  plot()

```

## 6.6.2 Quadrat Analysis

So as you saw in the lecture, we are interesting in knowing whether the distribution of points in our study area differs from ‘complete spatial randomness’ — CSR.

The most basic test of CSR is a quadrat analysis. We can carry out a simple quadrat analysis on our data using the quadrat count function in spatstat. 

Note, I wouldn’t recommend doing a quadrat analysis in any real piece of analysis you conduct, but it is useful for starting to understand the Poisson distribution…
```{r}

library("spatstat") # For simple quadrat analysis on our data using the quadrat count function

#First plot the points
plot(BluePlaquesSub.ppp,
     pch=16,
     cex=0.5, 
     main="Blue Plaques in Harrow")

#now count the points in that fall in a 6 x 6
#grid overlaid across the windowBluePlaquesSub.ppp2<-BluePlaquesSub.ppp %>%
BluePlaquesSub.ppp %>%
  quadratcount(.,nx = 6, ny = 6)%>%
    plot(., add=T, col="red")



```

###! Need to make sure you have run all the code leading up to the previous section plot() to run this figure. And need to make sure that you do not shut/close/remove the current plot after creating the first plot. 
Including: window <- as.owin(Harrow) 
           plot(window) 
I would suggest that you check what your BluePlaquesSub.ppp object looks like (see my console output when I look at this object). Then I suggest you check that the:
plot(BluePlaquesSub.ppp,
     pch=16,
     cex=0.5,
     main="Blue Plaques in Harrow")
gives the plot I have attached. The final bit of code then overlays the quadrat on top of this plot.



## In our case here, want to know whether or not there is any kind of spatial patterning associated with the Blue Plaques in areas of London. If you recall from the lecture, this means comparing our observed distribution of points with a statistically likely (Complete Spatial Random) distibution, based on the Poisson distribution.

Using the same quadratcount() function again (for the same sized grid) we can save the results into a table:

```{r}

#run the quadrat count
Qcount <- BluePlaquesSub.ppp %>%
  quadratcount(.,nx = 6, ny = 6) %>%
  as.data.frame() %>%
  dplyr::count(Var1=Freq)%>%
  dplyr::rename(Freqquadratcount=n)


## Check the data type in the first column — if it is factor, we will need to convert it to numeric

Qcount %>% 
  summarise_all(class)

```

OK, so we now have a frequency table — next we need to calculate our expected values. The formula for calculating expected probabilities based on the Poisson distribution is:



    x is the number of occurrences

    λ is the mean number of occurrences

    e is a constant- 2.718


```{r}
## now have a frequency table — next we need to calculate our expected values.

sums <- Qcount %>%
  #calculate the total blue plaques (Var * Freq)
  mutate(total = Var1 * Freqquadratcount) %>%
  dplyr::summarise(across(everything(), sum))%>%
  dplyr::select(-Var1) 

lambda<- Qcount%>%
  #calculate lambda
  mutate(total = Var1 * Freqquadratcount)%>%
  dplyr::summarise(across(everything(), sum)) %>%
  mutate(lambda=total/Freqquadratcount) %>%
  dplyr::select(lambda)%>%
  pull(lambda)



## Calculate expected using the Poisson formula from above 

QCountTable <- Qcount %>%
  mutate(Pr=((lambda^Var1)*exp(-lambda))/factorial(Var1))%>%
  #now calculate the expected counts based on our total number of plaques
  #and save them to the table
  mutate(Expected= (round(Pr * sums$Freqquadratcount, 0)))

#Compare the frequency distributions of the observed and expected point patterns
plot(c(1,5),c(0,14), type="n",
     xlab="Number of Blue Plaques (Red=Observed,Blue=Expected)",
     ylab="Frequency of Occurances",
     xaxt="n")
axis(side=1, at=1:5,labels=0:4) 
points(QCountTable$Freqquadratcount, 
       col="Red", 
       type="o", 
       lwd=3)
points(QCountTable$Expected, col="Blue", 
       type="o", 
       lwd=3)

```
the observed and expected frequencies for our quadrant counts, we can observe that they both have higher frequency counts at the lower end — something reminiscent of a Poisson distribution. This could indicate that for this particular set of quadrants, our pattern is close to Complete Spatial Randomness (i.e. no clustering or dispersal of points). But how do we confirm this?

## To check for sure, we can use the quadrat.test() function, built into spatstat. This uses a Chi Squared test to compare the observed and expected frequencies for each quadrant (rather than for quadrant bins, as we have just computed above).

A Chi-Squared test determines if there is an association between two categorical variables. The higher the Chi-Squared value, the greater the difference.

If the p-value of our Chi-Squared test is < 0.05, then we can reject a null hypothesis that says “there is no pattern - i.e. complete spatial randomness - in our data” (think of a null-hypothesis as the opposite of a hypothesis that says our data exhibit a pattern). What we need to look for is a value for p > 0.05. If our p-value is > 0.05 then this indicates that we have CSR and there is no pattern in our points. If it is < 0.05, this indicates that we do have clustering in our points.


```{r}

# Use Chi-Squared test to get p-value and to compare the observed and expected frequencies for each quadrant
teststats <- quadrat.test(BluePlaquesSub.ppp, nx = 6, ny = 6)

plot(BluePlaquesSub.ppp,pch=16,cex=0.5, main="Blue Plaques in Harrow")
plot(teststats, add=T, col = "red")

```

Here our p-value = 0.2594 (Open and view the teststats data), implying complete spatial randomness. But there are limitations of using qudrants like this…that we will explore next.

Note the warning message — some of the observed counts are very small (0) and this may affect the accuracy of the quadrant test. Recall that the Poisson distribution only describes observed occurrances that are counted in integers — where our occurrences = 0 (i.e. not observed), this can be an issue. We also know that there are various other problems that might affect our quadrat analysis, such as the modifiable areal unit problem.

In the new plot, we can see three figures for each quadrant. The top-left figure is the observed count of points; the top-right is the Poisson expected number of points; the bottom value is the residual value (also known as Pearson residual value), or (Observed - Expected) / Sqrt(Expected).




## 6.6.3 Try experimenting…
Try running a quadrant analysis for different grid arrangements (2 x 2, 3 x 3, 10 x 10 etc.) — how does this affect your results?

```{r}

library("spatstat") # For simple quadrat analysis on our data using the quadrat count function

#First plot the points
plot(BluePlaquesSub.ppp,
     pch=16,
     cex=0.5, 
     main="Blue Plaques in Harrow")

#now count the points in that fall in a 2 x 2
#grid overlaid across the windowBluePlaquesSub.ppp2<-BluePlaquesSub.ppp %>%
BluePlaquesSub.ppp %>%
  quadratcount(.,nx = 2, ny = 2)%>%
    plot(., add=T, col="red")

```

```{r}
#run the quadrat count
Qcount22 <- BluePlaquesSub.ppp %>%
  quadratcount(.,nx = 2, ny = 2) %>%
  as.data.frame() %>%
  dplyr::count(Var1=Freq)%>%
  dplyr::rename(Freqquadratcount=n)


## Check the data type in the first column — if it is factor, we will need to convert it to numeric

Qcount22 %>% 
  summarise_all(class)
```

```{r}
## now have a frequency table — next we need to calculate our expected values.

sums22 <- Qcount22 %>%
  #calculate the total blue plaques (Var * Freq)
  mutate(total = Var1 * Freqquadratcount) %>%
  dplyr::summarise(across(everything(), sum))%>%
  dplyr::select(-Var1) 

lambda22<- Qcount22%>%
  #calculate lambda
  mutate(total = Var1 * Freqquadratcount)%>%
  dplyr::summarise(across(everything(), sum)) %>%
  mutate(lambda=total/Freqquadratcount) %>%
  dplyr::select(lambda)%>%
  pull(lambda)



## Calculate expected using the Poisson formula from above 

QCountTable22 <- Qcount22 %>%
  mutate(Pr=((lambda^Var1)*exp(-lambda))/factorial(Var1))%>%
  #now calculate the expected counts based on our total number of plaques
  #and save them to the table
  mutate(Expected= (round(Pr * sums$Freqquadratcount, 0)))

#Compare the frequency distributions of the observed and expected point patterns
plot(c(1,5),c(0,14), type="n",
     xlab="Number of Blue Plaques (Red=Observed,Blue=Expected)",
     ylab="Frequency of Occurances",
     xaxt="n")
axis(side=1, at=1:5,labels=0:4) 
points(QCountTable22$Freqquadratcount, 
       col="Red", 
       type="o", 
       lwd=3)
points(QCountTable22$Expected, col="Blue", 
       type="o", 
       lwd=3)

```

```{r}
# Use Chi-Squared test to get p-value and to compare the observed and expected frequencies for each quadrant
teststats22 <- quadrat.test(BluePlaquesSub.ppp, nx = 2, ny = 2)

plot(BluePlaquesSub.ppp,pch=16,cex=0.5, main="Blue Plaques in Harrow")
plot(teststats22, add=T, col = "red")
```
Here our p-value = 0.8389




## 6.6.4 Ripley’s K
One way of getting around the limitations of quadrat analysis is to compare the observed distribution of points with the Poisson random model for a whole range of different distance radii. This is what Ripley’s K function computes.

We can conduct a Ripley’s K test on our data very simply with the spatstat package using the kest() function.


```{r}
# conduct a Ripley’s K test with the spatstat package using the kest() function.
K <- BluePlaquesSub.ppp %>%
  Kest(., correction="border") %>%
  plot()

Kval <- as.data.frame(Kest(BluePlaquesSub.ppp, correction = "Ripley"))

```

In English: Ripley’s K value for any circle radius r

= The average density of points for the entire study region (of all locations) λ=(n/Πr^2))

* Multiplied by the sum of the distances dij

* between all points within that search radius, see Dixon page 2 and Amgad et al. 2015

* Divided by the total number of points, n

* I = 1 or 0 depending if dij<r

The plot for K has a number of elements that are worth explaining. First, the Kpois(r) line in Red is the theoretical value of K for each distance window (r) under a Poisson assumption of Complete Spatial Randomness. The Black line is the estimated values of K accounting for the effects of the edge of the study area.

Here, the correction specifies how points towards the edge are dealt with, in this case, border means that points towards the edge are ignored for the calculation but are included for the central points. Section 2.1, here explains the different options.

Where the value of K falls above the line, the data appear to be clustered at that distance. Where the value of K is below the line, the data are dispersed. From the graph, we can see that up until distances of around 1300 metres, Blue Plaques appear to be clustered in Harrow, however, at around 1500 m, the distribution appears random and then dispersed between about 1600 and 2100 metres.


## 6.6.5 Alternatives to Ripley’s K
There are a number of alternative measures of spatial clustering which can be computed in spatstat such as the G and the L functions — I won’t go into them now, but if you are interested, you should delve into the following references.(Find it in the Week6 web)



## 6.7 Density-based spatial clustering of applications with noise: DBSCAN
Quadrat and Ripley’s K analysis are useful exploratory techniques for telling us if we have spatial clusters present in our point data, but they are not able to tell us WHERE in our area of interest the clusters are occurring. To discover this we need to use alternative techniques. One popular technique for discovering clusters in space (be this physical space or variable space) is DBSCAN. For the complete overview of the DBSCAN algorithm, read the original paper by Ester et al. (1996) or consult the wikipedia page

```{r}

library(fpc) #For DBSCAN: Density-based spatial clustering of applications with noise

```


# We will now carry out a DBSCAN analysis of blue plaques in my borough to see if there are any clusters present.
```{r}

#first check the coordinate reference system of the Harrow spatial polygon:
st_geometry(BoroughMap)

```

# DBSCAN requires you to input two parameters: 1. Epsilon - this is the radius within which the algorithm with search for clusters 2. MinPts - this is the minimum number of points that should be considered a cluster

Based on the results of the Ripley’s K analysis earlier, we can see that we are getting clustering up to a radius of around 1200m, with the largest bulge in the graph at around 700m. Therefore, 700m is probably a good place to start and we will begin by searching for clusters of at least 4 points…

```{r}

#first extract the points from the spatial points data frame
BluePlaquesSubPoints <- BluePlaquesSub %>%
  coordinates(.)%>%
  as.data.frame()

#now run the dbscan analysis
db <- BluePlaquesSubPoints %>%
  fpc::dbscan(.,eps = 700, MinPts = 4)

#now plot the results
plot(db, BluePlaquesSubPoints, main = "DBSCAN Output", frame = F)
plot(BoroughMap$geometry, add=T)


```

# You could also use kNNdistplot() from the dbscan pacakge to find a suitable eps value based on the ‘knee’ in the plot…

```{r}
# used to find suitable eps value based on the knee in plot
# k is no of nearest neighbours used, use min points

library(dbscan) # For used to find suitable eps value based on the knee in plot

BluePlaquesSubPoints%>%
  dbscan::kNNdistplot(.,k=4)



```
This plot shows for each point the average distance to the k neighbours, which are then plotted in ascending order. The knee is where this value (of distance to neighbours) increases. See this article from Data Novia for more information on this

So the DBSCAN analysis shows that for these values of eps and MinPts there are three clusters in the area I am analysing. Try varying eps and MinPts to see what difference it makes to the output.





Now of course the plot above is a little basic and doesn’t look very aesthetically pleasing. As this is R and R is brilliant, 
## we can always produce a much nicer plot by extracting the useful information from the DBSCAN output and use ggplot2 to produce a much cooler map…

```{r}
library(ggplot2)
```

Our new db object contains lots of info including the cluster each set of point coordinates belongs to, whether the point is a seed point or a border point etc. We can get a summary by just calling the object
```{r}
# get a summary by just calling the object
db

# If you open up the object in the environment window in RStudio, you will also see the various slots in the object, including cluster
db$cluster
```
Same data as the teach's data. Just different role order: look it from top to bottom, not right to left. 



```{r}

## We can now add this cluster membership info back into our dataframe
BluePlaquesSubPoints<- BluePlaquesSubPoints %>%
  mutate(dbcluster=db$cluster)

## Next we are going to create some convex hull polygons to wrap around the points in our clusters.
chulls <- BluePlaquesSubPoints %>%
  group_by(dbcluster) %>%
  dplyr::mutate(hull = 1:n(),
  hull = factor(hull, chull(coords.x1, coords.x2)))%>%
  arrange(hull)

#chulls2 <- ddply(BluePlaquesSubPoints, .(dbcluster), 
              #  function(df) df[chull(df$coords.x1, df$coords.x2), ])


## As 0 isn’t actually a cluster (it’s all points that aren’t in a cluster) drop it from the dataframe
chulls <- chulls %>%
  filter(dbcluster >=1)

```

# Now create a ggplot2 object from our data
```{r}
dbplot <- ggplot(data=BluePlaquesSubPoints, 
                 aes(coords.x1,coords.x2, colour=dbcluster, fill=dbcluster)) 
#add the points in
dbplot <- dbplot + geom_point()
#now the convex hulls
dbplot <- dbplot + geom_polygon(data = chulls, 
                                aes(coords.x1,coords.x2, group=dbcluster), 
                                alpha = 0.5) 
#now plot, setting the coordinates to scale correctly and as a black and white plot 
#(just for the hell of it)...
dbplot + theme_bw() + coord_equal()

```

Same cluster as the teach's graph, just different way to connect the points. 


# Now we are getting there, but wouldn’t it be better to add a basemap?!
```{r}

###add a basemap
##First get the bbox in lat long for Harrow
HarrowWGSbb <- Harrow %>%
  st_transform(., 4326)%>%
  st_bbox()

```

Now convert the basemap to British National Grid == 
```{r}
# install.packages("OpenStreetMap")
library(OpenStreetMap)

basemap <- OpenStreetMap::openmap(c(51.5549876,-0.4040502),c(51.6405356,-0.2671315),
                         zoom=NULL,
                         "osm")

  # convert the basemap to British National Grid
basemap_bng <- openproj(basemap, projection="+proj=tmerc +lat_0=49 +lon_0=-2 +k=0.9996012717 +x_0=400000 +y_0=-100000 +ellps=airy +nadgrids=OSTN15_NTv2_OSGBtoETRS.gsb +units=m +no_defs +type=crs")

```

Note this long string is the PROJ4 for British National Grid (EPSG 27700). In the past we could just specify “+init=epsg:27700”. However now we must find the PROJ4 on the EPSG website


# Now we can plot our fancy map with the clusters on…
```{r}

#autoplot(basemap_bng) sometimes works
autoplot.OpenStreetMap(basemap_bng)+ 
  geom_point(data=BluePlaquesSubPoints, 
             aes(coords.x1,coords.x2, 
                 colour=dbcluster, 
                 fill=dbcluster)) + 
  geom_polygon(data = chulls, 
               aes(coords.x1,coords.x2, 
                   group=dbcluster,
                   fill=dbcluster), 
               alpha = 0.5)  


```

